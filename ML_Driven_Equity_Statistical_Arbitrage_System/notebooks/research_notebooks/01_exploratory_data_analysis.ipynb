{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed43e20",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Statistical Arbitrage System\n",
    "\n",
    "**Purpose**: Comprehensive exploratory data analysis (EDA) for financial market data to understand data characteristics, identify patterns, and generate hypotheses for statistical arbitrage strategies.\n",
    "\n",
    "**Created**: June 25, 2025  \n",
    "**Author**: Statistical Arbitrage Research Team  \n",
    "**Version**: 1.0\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs initial data exploration and quality assessment for the statistical arbitrage system. We'll examine:\n",
    "\n",
    "1. **Data Loading**: Import various raw and processed datasets\n",
    "2. **Statistical Summaries**: Descriptive statistics and data quality checks\n",
    "3. **Data Visualization**: Charts and plots to understand distributions\n",
    "4. **Correlation Analysis**: Relationships between variables\n",
    "5. **Hypothesis Testing**: Initial statistical tests and model exploration\n",
    "6. **Key Findings**: Documentation of insights and next steps\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This is an exploratory notebook focused on discovery and understanding. For production models, refer to the structured notebooks in the `model_experimentation` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb3ad8",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports\n",
    "\n",
    "Import all necessary libraries for data analysis, visualization, and statistical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical libraries\n",
    "import scipy.stats as stats\n",
    "from scipy import signal\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Financial data libraries\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Time series analysis\n",
    "from datetime import datetime, timedelta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# System libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths\n",
    "project_root = Path.cwd().parent.parent\n",
    "data_dir = project_root / 'data'\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üìä Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35dff2c",
   "metadata": {},
   "source": [
    "## 2. Load Raw and Processed Datasets\n",
    "\n",
    "Load various financial datasets from different sources to understand data availability and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a937f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data():\n",
    "    \"\"\"Load sample financial data for analysis\"\"\"\n",
    "    # Define a list of sample stocks for pairs trading\n",
    "    stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'NFLX']\n",
    "    \n",
    "    # Date range for analysis\n",
    "    start_date = '2020-01-01'\n",
    "    end_date = '2024-12-31'\n",
    "    \n",
    "    print(f\"üì• Loading data for {len(stocks)} stocks from {start_date} to {end_date}\")\n",
    "    \n",
    "    try:\n",
    "        # Download stock data\n",
    "        data = yf.download(stocks, start=start_date, end=end_date)\n",
    "        print(f\"‚úÖ Successfully loaded data: {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    \"\"\"Generate synthetic financial data for testing\"\"\"\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2020-01-01', '2024-12-31', freq='D')\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # Generate correlated stock returns for pairs trading\n",
    "    returns = np.random.multivariate_normal(\n",
    "        mean=[0.0005, 0.0005],  # Small positive drift\n",
    "        cov=[[0.0004, 0.0002],  # Correlation between assets\n",
    "             [0.0002, 0.0004]], \n",
    "        size=n_days\n",
    "    )\n",
    "    \n",
    "    # Convert to prices\n",
    "    prices = np.cumprod(1 + returns, axis=0) * 100\n",
    "    \n",
    "    synthetic_data = pd.DataFrame(\n",
    "        prices, \n",
    "        index=dates, \n",
    "        columns=['Stock_A', 'Stock_B']\n",
    "    )\n",
    "    \n",
    "    print(f\"üîß Generated synthetic data: {synthetic_data.shape}\")\n",
    "    return synthetic_data\n",
    "\n",
    "# Load real market data\n",
    "market_data = load_sample_data()\n",
    "\n",
    "# Generate synthetic data as fallback\n",
    "synthetic_data = generate_synthetic_data()\n",
    "\n",
    "# Check data availability\n",
    "if market_data is not None:\n",
    "    print(f\"\\nüìä Market Data Overview:\")\n",
    "    print(f\"Shape: {market_data.shape}\")\n",
    "    print(f\"Date range: {market_data.index[0]} to {market_data.index[-1]}\")\n",
    "    print(f\"Columns: {market_data.columns.get_level_values(0).unique().tolist()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using synthetic data for analysis\")\n",
    "    market_data = synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and initial inspection\n",
    "def preprocess_market_data(data):\n",
    "    \"\"\"Clean and preprocess market data\"\"\"\n",
    "    if data.columns.nlevels > 1:\n",
    "        # Extract adjusted close prices for multi-level columns\n",
    "        if 'Adj Close' in data.columns.get_level_values(0):\n",
    "            prices = data['Adj Close'].copy()\n",
    "        else:\n",
    "            prices = data['Close'].copy()\n",
    "    else:\n",
    "        prices = data.copy()\n",
    "    \n",
    "    # Forward fill missing values\n",
    "    prices = prices.fillna(method='ffill')\n",
    "    \n",
    "    # Calculate returns\n",
    "    returns = prices.pct_change().dropna()\n",
    "    \n",
    "    # Calculate log returns\n",
    "    log_returns = np.log(prices / prices.shift(1)).dropna()\n",
    "    \n",
    "    return {\n",
    "        'prices': prices,\n",
    "        'returns': returns,\n",
    "        'log_returns': log_returns\n",
    "    }\n",
    "\n",
    "# Preprocess the data\n",
    "processed_data = preprocess_market_data(market_data)\n",
    "prices = processed_data['prices']\n",
    "returns = processed_data['returns']\n",
    "log_returns = processed_data['log_returns']\n",
    "\n",
    "print(\"üìà Data Preprocessing Complete:\")\n",
    "print(f\"Prices shape: {prices.shape}\")\n",
    "print(f\"Returns shape: {returns.shape}\")\n",
    "print(f\"Log returns shape: {log_returns.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nüìã First 5 rows of prices:\")\n",
    "print(prices.head())\n",
    "\n",
    "print(f\"\\nüìã Last 5 rows of returns:\")\n",
    "print(returns.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ea83f",
   "metadata": {},
   "source": [
    "## 3. Statistical Summaries of Data\n",
    "\n",
    "Generate comprehensive descriptive statistics to understand data characteristics, distributions, and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3059b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_stats(data, data_type=\"returns\"):\n",
    "    \"\"\"Generate comprehensive statistical summary\"\"\"\n",
    "    stats_dict = {}\n",
    "    \n",
    "    for column in data.columns:\n",
    "        series = data[column].dropna()\n",
    "        \n",
    "        stats_dict[column] = {\n",
    "            'count': len(series),\n",
    "            'mean': series.mean(),\n",
    "            'std': series.std(),\n",
    "            'min': series.min(),\n",
    "            '25%': series.quantile(0.25),\n",
    "            '50%': series.median(),\n",
    "            '75%': series.quantile(0.75),\n",
    "            'max': series.max(),\n",
    "            'skewness': series.skew(),\n",
    "            'kurtosis': series.kurtosis(),\n",
    "            'jarque_bera': stats.jarque_bera(series)[1],  # p-value\n",
    "            'missing_values': data[column].isna().sum(),\n",
    "            'missing_pct': (data[column].isna().sum() / len(data)) * 100\n",
    "        }\n",
    "        \n",
    "        # Additional financial metrics for returns\n",
    "        if data_type == \"returns\":\n",
    "            stats_dict[column].update({\n",
    "                'sharpe_ratio': series.mean() / series.std() * np.sqrt(252),\n",
    "                'var_95': series.quantile(0.05),\n",
    "                'var_99': series.quantile(0.01),\n",
    "                'max_drawdown': (series.cumsum() - series.cumsum().expanding().max()).min()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(stats_dict).round(6)\n",
    "\n",
    "# Generate statistics for prices\n",
    "print(\"üìä PRICE STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "price_stats = generate_comprehensive_stats(prices, \"prices\")\n",
    "print(price_stats.T.head(10))\n",
    "\n",
    "print(\"\\nüìä RETURNS STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "return_stats = generate_comprehensive_stats(returns, \"returns\")\n",
    "print(return_stats.T)\n",
    "\n",
    "print(\"\\nüìä LOG RETURNS STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "log_return_stats = generate_comprehensive_stats(log_returns, \"returns\")\n",
    "print(log_return_stats.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "def assess_data_quality(data, name=\"Dataset\"):\n",
    "    \"\"\"Perform comprehensive data quality assessment\"\"\"\n",
    "    print(f\"\\nüîç DATA QUALITY ASSESSMENT: {name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "    print(f\"Total trading days: {len(data)}\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = data.isnull().sum()\n",
    "    missing_pct = (missing_data / len(data)) * 100\n",
    "    \n",
    "    print(f\"\\nüìâ Missing Values:\")\n",
    "    for col in data.columns:\n",
    "        if missing_data[col] > 0:\n",
    "            print(f\"  {col}: {missing_data[col]} ({missing_pct[col]:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"  {col}: No missing values ‚úÖ\")\n",
    "    \n",
    "    # Outlier detection (using IQR method)\n",
    "    print(f\"\\nüéØ Outlier Detection (IQR method):\")\n",
    "    for col in data.columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)][col]\n",
    "        print(f\"  {col}: {len(outliers)} outliers ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "    \n",
    "    # Data consistency checks\n",
    "    print(f\"\\nüîß Data Consistency:\")\n",
    "    print(f\"  Duplicated timestamps: {data.index.duplicated().sum()}\")\n",
    "    print(f\"  Sorted chronologically: {data.index.is_monotonic_increasing}\")\n",
    "    \n",
    "    return {\n",
    "        'missing_data': missing_data,\n",
    "        'missing_pct': missing_pct,\n",
    "        'outliers_detected': True\n",
    "    }\n",
    "\n",
    "# Assess quality for each dataset\n",
    "price_quality = assess_data_quality(prices, \"Stock Prices\")\n",
    "return_quality = assess_data_quality(returns, \"Stock Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a77109",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n",
    "\n",
    "Create comprehensive visualizations to understand data distributions, trends, and potential anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Trends Visualization\n",
    "def plot_price_trends(data, title=\"Stock Price Trends\"):\n",
    "    \"\"\"Plot price trends for all stocks\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=['Normalized Prices (Base=100)', 'Absolute Prices'],\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Normalize prices to base 100\n",
    "    normalized_data = data.div(data.iloc[0]) * 100\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    for i, col in enumerate(data.columns):\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Normalized prices\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=normalized_data.index,\n",
    "                y=normalized_data[col],\n",
    "                name=f\"{col} (Norm)\",\n",
    "                line=dict(color=color),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Absolute prices\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data[col],\n",
    "                name=f\"{col} (Abs)\",\n",
    "                line=dict(color=color, dash='dash'),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=800,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Normalized Price\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Absolute Price ($)\", row=2, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot price trends\n",
    "price_fig = plot_price_trends(prices)\n",
    "price_fig.show()\n",
    "\n",
    "print(\"üìà Price trends plotted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90543db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Distribution Analysis\n",
    "def plot_returns_distribution(returns_data, title=\"Returns Distribution Analysis\"):\n",
    "    \"\"\"Create comprehensive returns distribution plots\"\"\"\n",
    "    n_stocks = len(returns_data.columns)\n",
    "    n_cols = min(3, n_stocks)\n",
    "    n_rows = (n_stocks + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_stocks == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(returns_data.columns):\n",
    "        data = returns_data[col].dropna()\n",
    "        \n",
    "        # Create histogram with normal distribution overlay\n",
    "        axes[i].hist(data, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        \n",
    "        # Fit normal distribution\n",
    "        mu, sigma = stats.norm.fit(data)\n",
    "        x = np.linspace(data.min(), data.max(), 100)\n",
    "        axes[i].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label=f'Normal (Œº={mu:.4f}, œÉ={sigma:.4f})')\n",
    "        \n",
    "        # Add statistics to plot\n",
    "        axes[i].axvline(data.mean(), color='red', linestyle='--', alpha=0.8, label=f'Mean: {data.mean():.4f}')\n",
    "        axes[i].axvline(data.median(), color='green', linestyle='--', alpha=0.8, label=f'Median: {data.median():.4f}')\n",
    "        \n",
    "        axes[i].set_title(f'{col} - Returns Distribution')\n",
    "        axes[i].set_xlabel('Daily Returns')\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_stocks, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Plot returns distributions\n",
    "plot_returns_distribution(returns, \"Daily Returns Distribution\")\n",
    "\n",
    "# Summary statistics in a nice format\n",
    "print(\"\\nüìä RETURNS DISTRIBUTION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for col in returns.columns:\n",
    "    data = returns[col].dropna()\n",
    "    jb_stat, jb_pvalue = stats.jarque_bera(data)\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Mean: {data.mean():.6f}\")\n",
    "    print(f\"  Std Dev: {data.std():.6f}\")\n",
    "    print(f\"  Skewness: {data.skew():.4f}\")\n",
    "    print(f\"  Kurtosis: {data.kurtosis():.4f}\")\n",
    "    print(f\"  Jarque-Bera p-value: {jb_pvalue:.6f}\")\n",
    "    print(f\"  Normal distribution: {'No ‚ùå' if jb_pvalue < 0.05 else 'Yes ‚úÖ'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Analysis and Rolling Statistics\n",
    "def plot_volatility_analysis(returns_data, window=30):\n",
    "    \"\"\"Plot rolling volatility and other time-varying statistics\"\"\"\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    rolling_mean = returns_data.rolling(window=window).mean()\n",
    "    rolling_std = returns_data.rolling(window=window).std()\n",
    "    rolling_vol = rolling_std * np.sqrt(252)  # Annualized volatility\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=[\n",
    "            f'Rolling {window}-Day Mean Returns',\n",
    "            f'Rolling {window}-Day Volatility (Annualized)',\n",
    "            'Cumulative Returns'\n",
    "        ],\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    for i, col in enumerate(returns_data.columns):\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Rolling mean\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rolling_mean.index,\n",
    "                y=rolling_mean[col],\n",
    "                name=f\"{col} Mean\",\n",
    "                line=dict(color=color),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Rolling volatility\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rolling_vol.index,\n",
    "                y=rolling_vol[col],\n",
    "                name=f\"{col} Vol\",\n",
    "                line=dict(color=color),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Cumulative returns\n",
    "        cum_returns = (1 + returns_data[col]).cumprod() - 1\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=cum_returns.index,\n",
    "                y=cum_returns,\n",
    "                name=f\"{col} Cum\",\n",
    "                line=dict(color=color),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Volatility and Performance Analysis\",\n",
    "        height=900,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Rolling Mean\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Annualized Volatility\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Cumulative Returns\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=3, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot volatility analysis\n",
    "vol_fig = plot_volatility_analysis(returns, window=30)\n",
    "vol_fig.show()\n",
    "\n",
    "# Calculate and display volatility statistics\n",
    "print(\"\\nüìä VOLATILITY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "annual_vol = returns.std() * np.sqrt(252)\n",
    "for col in returns.columns:\n",
    "    vol = annual_vol[col]\n",
    "    print(f\"{col}: {vol:.2%} annual volatility\")\n",
    "\n",
    "print(f\"\\nAverage volatility: {annual_vol.mean():.2%}\")\n",
    "print(f\"Volatility range: {annual_vol.min():.2%} - {annual_vol.max():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b309e78",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis\n",
    "\n",
    "Analyze relationships between different assets to identify potential pairs trading opportunities and portfolio diversification benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694322cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "def analyze_correlations(data, title=\"Correlation Analysis\"):\n",
    "    \"\"\"Comprehensive correlation analysis\"\"\"\n",
    "    \n",
    "    # Calculate correlation matrices\n",
    "    price_corr = prices.corr()\n",
    "    return_corr = returns.corr()\n",
    "    \n",
    "    # Create correlation heatmaps\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Price correlations\n",
    "    sns.heatmap(price_corr, annot=True, cmap='RdBu_r', center=0, \n",
    "                square=True, ax=axes[0], cbar_kws={'label': 'Correlation'})\n",
    "    axes[0].set_title('Price Correlations')\n",
    "    \n",
    "    # Return correlations\n",
    "    sns.heatmap(return_corr, annot=True, cmap='RdBu_r', center=0, \n",
    "                square=True, ax=axes[1], cbar_kws={'label': 'Correlation'})\n",
    "    axes[1].set_title('Return Correlations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return price_corr, return_corr\n",
    "\n",
    "# Perform correlation analysis\n",
    "price_corr, return_corr = analyze_correlations(returns)\n",
    "\n",
    "# Find highly correlated pairs for potential pairs trading\n",
    "def find_correlation_pairs(corr_matrix, threshold=0.7):\n",
    "    \"\"\"Find highly correlated asset pairs\"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr = corr_matrix.iloc[i, j]\n",
    "            if abs(corr) >= threshold:\n",
    "                pairs.append({\n",
    "                    'asset1': corr_matrix.columns[i],\n",
    "                    'asset2': corr_matrix.columns[j],\n",
    "                    'correlation': corr\n",
    "                })\n",
    "    return pd.DataFrame(pairs).sort_values('correlation', key=abs, ascending=False)\n",
    "\n",
    "print(\"üîó HIGHLY CORRELATED PAIRS (|correlation| >= 0.7)\")\n",
    "print(\"=\" * 60)\n",
    "corr_pairs = find_correlation_pairs(return_corr, threshold=0.7)\n",
    "if len(corr_pairs) > 0:\n",
    "    print(corr_pairs)\n",
    "else:\n",
    "    print(\"No pairs found with correlation >= 0.7\")\n",
    "\n",
    "# Rolling correlation analysis\n",
    "def plot_rolling_correlations(data1, data2, window=60, title=\"Rolling Correlation\"):\n",
    "    \"\"\"Plot rolling correlation between two assets\"\"\"\n",
    "    if len(data1.columns) >= 2:\n",
    "        asset1 = data1.columns[0]\n",
    "        asset2 = data1.columns[1] if len(data1.columns) > 1 else data1.columns[0]\n",
    "        \n",
    "        rolling_corr = data1[asset1].rolling(window=window).corr(data1[asset2])\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=rolling_corr.index,\n",
    "            y=rolling_corr,\n",
    "            mode='lines',\n",
    "            name=f'{asset1} vs {asset2}',\n",
    "            line=dict(color='blue', width=2)\n",
    "        ))\n",
    "        \n",
    "        fig.add_hline(y=0.7, line_dash=\"dash\", line_color=\"red\", \n",
    "                     annotation_text=\"High Correlation (0.7)\")\n",
    "        fig.add_hline(y=-0.7, line_dash=\"dash\", line_color=\"red\")\n",
    "        fig.add_hline(y=0, line_dash=\"dot\", line_color=\"gray\")\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"{title} ({window}-day window)\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Rolling Correlation\",\n",
    "            hovermode='x'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    return None\n",
    "\n",
    "# Plot rolling correlations for first two assets\n",
    "if len(returns.columns) >= 2:\n",
    "    rolling_corr_fig = plot_rolling_correlations(returns, returns, window=60)\n",
    "    if rolling_corr_fig:\n",
    "        rolling_corr_fig.show()\n",
    "\n",
    "print(f\"\\nüìä CORRELATION SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Average return correlation: {return_corr.values[np.triu_indices_from(return_corr.values, k=1)].mean():.4f}\")\n",
    "print(f\"Max return correlation: {return_corr.values[np.triu_indices_from(return_corr.values, k=1)].max():.4f}\")\n",
    "print(f\"Min return correlation: {return_corr.values[np.triu_indices_from(return_corr.values, k=1)].min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab345e9",
   "metadata": {},
   "source": [
    "## 6. Test Simple Hypotheses\n",
    "\n",
    "Formulate and test basic statistical hypotheses about the data to validate assumptions for trading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing Framework\n",
    "def test_statistical_hypotheses(data, alpha=0.05):\n",
    "    \"\"\"Test various statistical hypotheses relevant to trading\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for column in data.columns:\n",
    "        series = data[column].dropna()\n",
    "        column_results = {}\n",
    "        \n",
    "        # H0: Returns are normally distributed\n",
    "        jb_stat, jb_pvalue = stats.jarque_bera(series)\n",
    "        column_results['normality'] = {\n",
    "            'test': 'Jarque-Bera',\n",
    "            'statistic': jb_stat,\n",
    "            'p_value': jb_pvalue,\n",
    "            'reject_h0': jb_pvalue < alpha,\n",
    "            'interpretation': 'Returns are NOT normally distributed' if jb_pvalue < alpha else 'Returns are normally distributed'\n",
    "        }\n",
    "        \n",
    "        # H0: Mean return = 0 (no drift)\n",
    "        t_stat, t_pvalue = stats.ttest_1samp(series, 0)\n",
    "        column_results['zero_mean'] = {\n",
    "            'test': 'One-sample t-test',\n",
    "            'statistic': t_stat,\n",
    "            'p_value': t_pvalue,\n",
    "            'reject_h0': t_pvalue < alpha,\n",
    "            'interpretation': 'Mean return significantly different from 0' if t_pvalue < alpha else 'Mean return not significantly different from 0'\n",
    "        }\n",
    "        \n",
    "        # H0: No autocorrelation (returns are independent)\n",
    "        try:\n",
    "            ljung_stat, ljung_pvalue = acorr_ljungbox(series, lags=10, return_df=False)\n",
    "            column_results['autocorrelation'] = {\n",
    "                'test': 'Ljung-Box',\n",
    "                'statistic': ljung_stat[9],  # 10th lag\n",
    "                'p_value': ljung_pvalue[9],\n",
    "                'reject_h0': ljung_pvalue[9] < alpha,\n",
    "                'interpretation': 'Significant autocorrelation detected' if ljung_pvalue[9] < alpha else 'No significant autocorrelation'\n",
    "            }\n",
    "        except:\n",
    "            column_results['autocorrelation'] = {'test': 'Ljung-Box', 'error': 'Could not compute'}\n",
    "        \n",
    "        # H0: Data is stationary\n",
    "        try:\n",
    "            adf_stat, adf_pvalue, _, _, adf_critical, _ = adfuller(series)\n",
    "            column_results['stationarity'] = {\n",
    "                'test': 'Augmented Dickey-Fuller',\n",
    "                'statistic': adf_stat,\n",
    "                'p_value': adf_pvalue,\n",
    "                'critical_values': adf_critical,\n",
    "                'reject_h0': adf_pvalue < alpha,\n",
    "                'interpretation': 'Series is stationary' if adf_pvalue < alpha else 'Series has unit root (non-stationary)'\n",
    "            }\n",
    "        except:\n",
    "            column_results['stationarity'] = {'test': 'ADF', 'error': 'Could not compute'}\n",
    "        \n",
    "        results[column] = column_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run hypothesis tests\n",
    "print(\"üß™ STATISTICAL HYPOTHESIS TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "hypothesis_results = test_statistical_hypotheses(returns)\n",
    "\n",
    "for asset, tests in hypothesis_results.items():\n",
    "    print(f\"\\nüìä {asset}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for test_name, result in tests.items():\n",
    "        if 'error' not in result:\n",
    "            print(f\"{test_name.title()} Test ({result['test']}):\")\n",
    "            print(f\"  Statistic: {result['statistic']:.4f}\")\n",
    "            print(f\"  P-value: {result['p_value']:.6f}\")\n",
    "            print(f\"  Result: {result['interpretation']}\")\n",
    "            print()\n",
    "\n",
    "# Cointegration test for pairs trading\n",
    "def test_cointegration(data, alpha=0.05):\n",
    "    \"\"\"Test for cointegration between asset pairs\"\"\"\n",
    "    print(f\"\\nüîó COINTEGRATION TESTING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    cointegration_results = []\n",
    "    \n",
    "    for i in range(len(data.columns)):\n",
    "        for j in range(i+1, len(data.columns)):\n",
    "            asset1, asset2 = data.columns[i], data.columns[j]\n",
    "            \n",
    "            try:\n",
    "                # Cointegration test\n",
    "                score, pvalue, crit_values = coint(data[asset1], data[asset2])\n",
    "                \n",
    "                result = {\n",
    "                    'asset1': asset1,\n",
    "                    'asset2': asset2,\n",
    "                    'score': score,\n",
    "                    'p_value': pvalue,\n",
    "                    'critical_values': crit_values,\n",
    "                    'cointegrated': pvalue < alpha\n",
    "                }\n",
    "                \n",
    "                cointegration_results.append(result)\n",
    "                \n",
    "                print(f\"{asset1} - {asset2}:\")\n",
    "                print(f\"  Score: {score:.4f}\")\n",
    "                print(f\"  P-value: {pvalue:.4f}\")\n",
    "                print(f\"  Cointegrated: {'Yes ‚úÖ' if pvalue < alpha else 'No ‚ùå'}\")\n",
    "                print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{asset1} - {asset2}: Error - {e}\")\n",
    "    \n",
    "    return cointegration_results\n",
    "\n",
    "# Test cointegration if we have enough assets\n",
    "if len(prices.columns) >= 2:\n",
    "    coint_results = test_cointegration(prices)\n",
    "else:\n",
    "    print(\"Need at least 2 assets for cointegration testing\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0232ad65",
   "metadata": {},
   "source": [
    "# Advanced Exploratory Data Analysis for Statistical Arbitrage\n",
    "\n",
    "This notebook focuses on advanced exploratory data analysis (EDA) techniques for developing statistical arbitrage strategies. We'll explore data characteristics, test hypotheses, and uncover insights that can inform our alpha generation models.\n",
    "\n",
    "## Objectives\n",
    "- Load and examine multiple financial datasets\n",
    "- Perform comprehensive statistical analysis\n",
    "- Test hypotheses related to market inefficiencies  \n",
    "- Identify potential alpha signals through data exploration\n",
    "- Document findings for model development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd87e94",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973090cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, jarque_bera, kstest\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Financial data libraries\n",
    "import yfinance as yf\n",
    "from arch import arch_model\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8f1f8",
   "metadata": {},
   "source": [
    "## 2. Load Raw and Processed Datasets\n",
    "\n",
    "We'll load various financial datasets including stock prices, economic indicators, and alternative data sources to explore potential statistical arbitrage opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9174e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data paths\n",
    "RAW_DATA_PATH = \"../../data/raw/\"\n",
    "PROCESSED_DATA_PATH = \"../../data/processed/\"\n",
    "TEST_DATA_PATH = \"../../data/test/\"\n",
    "\n",
    "# Sample tickers for analysis - focusing on sector pairs for statistical arbitrage\n",
    "tech_stocks = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'TSLA']\n",
    "financial_stocks = ['JPM', 'BAC', 'WFC', 'GS', 'MS']\n",
    "energy_stocks = ['XOM', 'CVX', 'COP', 'EOG', 'SLB']\n",
    "\n",
    "all_tickers = tech_stocks + financial_stocks + energy_stocks\n",
    "\n",
    "# Download recent price data for analysis\n",
    "print(\"Downloading market data...\")\n",
    "try:\n",
    "    # Get 2 years of daily data\n",
    "    price_data = yf.download(all_tickers, period=\"2y\", interval=\"1d\")['Adj Close']\n",
    "    \n",
    "    # Handle single vs multiple tickers\n",
    "    if len(all_tickers) == 1:\n",
    "        price_data = price_data.to_frame(all_tickers[0])\n",
    "    \n",
    "    print(f\"Successfully downloaded data for {len(all_tickers)} stocks\")\n",
    "    print(f\"Date range: {price_data.index.min()} to {price_data.index.max()}\")\n",
    "    print(f\"Data shape: {price_data.shape}\")\n",
    "    \n",
    "    # Calculate returns\n",
    "    returns = price_data.pct_change().dropna()\n",
    "    log_returns = np.log(price_data / price_data.shift(1)).dropna()\n",
    "    \n",
    "    print(f\"Returns data shape: {returns.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {e}\")\n",
    "    # Create synthetic data for demonstration\n",
    "    dates = pd.date_range(start='2022-01-01', end='2024-01-01', freq='D')\n",
    "    np.random.seed(42)\n",
    "    price_data = pd.DataFrame({\n",
    "        ticker: 100 * np.exp(np.cumsum(np.random.normal(0.0005, 0.02, len(dates))))\n",
    "        for ticker in all_tickers\n",
    "    }, index=dates)\n",
    "    returns = price_data.pct_change().dropna()\n",
    "    log_returns = np.log(price_data / price_data.shift(1)).dropna()\n",
    "    print(\"Using synthetic data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fccf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional market data - VIX for volatility analysis\n",
    "print(\"Loading additional market indicators...\")\n",
    "try:\n",
    "    # VIX for market volatility\n",
    "    vix_data = yf.download(\"^VIX\", period=\"2y\", interval=\"1d\")['Adj Close']\n",
    "    \n",
    "    # Treasury rates for risk-free rate\n",
    "    treasury_10y = yf.download(\"^TNX\", period=\"2y\", interval=\"1d\")['Adj Close']\n",
    "    \n",
    "    # Dollar index for currency effects\n",
    "    dxy = yf.download(\"DX-Y.NYB\", period=\"2y\", interval=\"1d\")['Adj Close']\n",
    "    \n",
    "    # Create market factors dataframe\n",
    "    market_factors = pd.DataFrame({\n",
    "        'VIX': vix_data,\n",
    "        'Treasury_10Y': treasury_10y,\n",
    "        'DXY': dxy\n",
    "    }).dropna()\n",
    "    \n",
    "    print(f\"Market factors data shape: {market_factors.shape}\")\n",
    "    print(\"Market factors loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading market factors: {e}\")\n",
    "    # Create synthetic market factors\n",
    "    market_factors = pd.DataFrame({\n",
    "        'VIX': 20 + 10 * np.random.randn(len(price_data)),\n",
    "        'Treasury_10Y': 2.5 + 0.5 * np.random.randn(len(price_data)),\n",
    "        'DXY': 100 + 5 * np.random.randn(len(price_data))\n",
    "    }, index=price_data.index)\n",
    "    print(\"Using synthetic market factors\")\n",
    "\n",
    "# Display basic info about loaded datasets\n",
    "print(\"\\n=== Dataset Summary ===\")\n",
    "print(f\"Price data: {price_data.shape}\")\n",
    "print(f\"Returns data: {returns.shape}\")\n",
    "print(f\"Market factors: {market_factors.shape}\")\n",
    "print(f\"Date range: {price_data.index.min()} to {price_data.index.max()}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\n=== Sample Price Data ===\")\n",
    "print(price_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b97e1",
   "metadata": {},
   "source": [
    "## 3. Statistical Summaries of Data\n",
    "\n",
    "Comprehensive statistical analysis of the loaded datasets to understand their distributional properties, moments, and key characteristics relevant for statistical arbitrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive statistics for returns\n",
    "print(\"=== RETURNS DESCRIPTIVE STATISTICS ===\")\n",
    "returns_stats = returns.describe()\n",
    "print(returns_stats)\n",
    "\n",
    "# Calculate additional moments and risk metrics\n",
    "def calculate_advanced_stats(data):\n",
    "    \"\"\"Calculate advanced statistical measures\"\"\"\n",
    "    stats_dict = {}\n",
    "    \n",
    "    for col in data.columns:\n",
    "        series = data[col].dropna()\n",
    "        stats_dict[col] = {\n",
    "            'Mean': series.mean(),\n",
    "            'Std': series.std(),\n",
    "            'Skewness': stats.skew(series),\n",
    "            'Kurtosis': stats.kurtosis(series),\n",
    "            'Jarque-Bera': stats.jarque_bera(series)[0],\n",
    "            'JB p-value': stats.jarque_bera(series)[1],\n",
    "            'Sharpe Ratio': series.mean() / series.std() * np.sqrt(252),\n",
    "            'VaR_5%': np.percentile(series, 5),\n",
    "            'CVaR_5%': series[series <= np.percentile(series, 5)].mean(),\n",
    "            'Max Drawdown': (series.cumsum() - series.cumsum().expanding().max()).min()\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(stats_dict).T\n",
    "\n",
    "# Calculate advanced statistics\n",
    "advanced_stats = calculate_advanced_stats(returns)\n",
    "print(\"\\n=== ADVANCED STATISTICAL MEASURES ===\")\n",
    "print(advanced_stats.round(4))\n",
    "\n",
    "# Sector-wise analysis\n",
    "print(\"\\n=== SECTOR-WISE ANALYSIS ===\")\n",
    "sectors = {\n",
    "    'Technology': tech_stocks,\n",
    "    'Financial': financial_stocks,\n",
    "    'Energy': energy_stocks\n",
    "}\n",
    "\n",
    "for sector, stocks in sectors.items():\n",
    "    sector_returns = returns[stocks]\n",
    "    sector_mean = sector_returns.mean().mean()\n",
    "    sector_vol = sector_returns.std().mean()\n",
    "    sector_corr = sector_returns.corr().mean().mean()\n",
    "    \n",
    "    print(f\"{sector}:\")\n",
    "    print(f\"  Average Return: {sector_mean:.4f}\")\n",
    "    print(f\"  Average Volatility: {sector_vol:.4f}\")\n",
    "    print(f\"  Average Correlation: {sector_corr:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b14ba8",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n",
    "\n",
    "Comprehensive visualization of the financial data to identify patterns, distributions, and relationships that may inform statistical arbitrage strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Price Evolution Over Time\n",
    "fig = make_subplots(rows=3, cols=1, \n",
    "                    subplot_titles=['Technology Stocks', 'Financial Stocks', 'Energy Stocks'],\n",
    "                    vertical_spacing=0.08)\n",
    "\n",
    "# Normalize prices to start at 100 for comparison\n",
    "normalized_prices = price_data.div(price_data.iloc[0]) * 100\n",
    "\n",
    "# Plot each sector\n",
    "sectors_data = [\n",
    "    (tech_stocks, 1, 'Technology'),\n",
    "    (financial_stocks, 2, 'Financial'), \n",
    "    (energy_stocks, 3, 'Energy')\n",
    "]\n",
    "\n",
    "colors = px.colors.qualitative.Set1\n",
    "\n",
    "for stocks, row, sector in sectors_data:\n",
    "    for i, stock in enumerate(stocks):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=normalized_prices.index, \n",
    "                      y=normalized_prices[stock],\n",
    "                      name=f\"{stock}\",\n",
    "                      line=dict(color=colors[i % len(colors)]),\n",
    "                      showlegend=(row==1)),\n",
    "            row=row, col=1\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=800, title=\"Normalized Price Evolution by Sector\")\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"Normalized Price (Base=100)\")\n",
    "fig.show()\n",
    "\n",
    "# 4.2 Returns Distribution Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution of daily returns\n",
    "returns.plot(kind='hist', bins=50, alpha=0.7, ax=axes[0,0])\n",
    "axes[0,0].set_title('Distribution of Daily Returns')\n",
    "axes[0,0].set_xlabel('Daily Returns')\n",
    "axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Box plot of returns by sector\n",
    "sector_returns = []\n",
    "sector_names = []\n",
    "for sector, stocks in sectors.items():\n",
    "    for stock in stocks:\n",
    "        sector_returns.extend(returns[stock].dropna())\n",
    "        sector_names.extend([sector] * len(returns[stock].dropna()))\n",
    "\n",
    "returns_df = pd.DataFrame({'Returns': sector_returns, 'Sector': sector_names})\n",
    "sns.boxplot(data=returns_df, x='Sector', y='Returns', ax=axes[0,1])\n",
    "axes[0,1].set_title('Returns Distribution by Sector')\n",
    "\n",
    "# Q-Q plot for normality check (using first stock as example)\n",
    "stats.probplot(returns[all_tickers[0]].dropna(), dist=\"norm\", plot=axes[1,0])\n",
    "axes[1,0].set_title(f'Q-Q Plot: {all_tickers[0]} Returns vs Normal Distribution')\n",
    "\n",
    "# Rolling volatility\n",
    "rolling_vol = returns.rolling(window=30).std() * np.sqrt(252)\n",
    "rolling_vol.plot(ax=axes[1,1], alpha=0.7)\n",
    "axes[1,1].set_title('30-Day Rolling Volatility (Annualized)')\n",
    "axes[1,1].set_ylabel('Volatility')\n",
    "axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Scatter Plot Matrix for Sector Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (sector, stocks) in enumerate(sectors.items()):\n",
    "    if len(stocks) >= 2:\n",
    "        # Create scatter plot for first two stocks in each sector\n",
    "        stock1, stock2 = stocks[0], stocks[1]\n",
    "        axes[i].scatter(returns[stock1], returns[stock2], alpha=0.6, s=20)\n",
    "        axes[i].set_xlabel(f'{stock1} Returns')\n",
    "        axes[i].set_ylabel(f'{stock2} Returns')\n",
    "        axes[i].set_title(f'{sector} Sector: {stock1} vs {stock2}')\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        corr = returns[stock1].corr(returns[stock2])\n",
    "        axes[i].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                    transform=axes[i].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.4 Volatility Clustering Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Select representative stocks for volatility analysis\n",
    "vol_stocks = [tech_stocks[0], financial_stocks[0], energy_stocks[0]]\n",
    "\n",
    "for i, stock in enumerate(vol_stocks):\n",
    "    row, col = i // 2, i % 2\n",
    "    \n",
    "    # Plot returns\n",
    "    axes[row, col].plot(returns.index, returns[stock], alpha=0.7, linewidth=0.8)\n",
    "    axes[row, col].set_title(f'{stock} Daily Returns')\n",
    "    axes[row, col].set_ylabel('Returns')\n",
    "    \n",
    "    # Add volatility regime highlighting\n",
    "    vol_series = returns[stock].rolling(30).std()\n",
    "    high_vol_periods = vol_series > vol_series.quantile(0.8)\n",
    "    \n",
    "    # Highlight high volatility periods\n",
    "    for j, (date, high_vol) in enumerate(high_vol_periods.items()):\n",
    "        if high_vol and j < len(returns):\n",
    "            axes[row, col].axvspan(date, date, alpha=0.3, color='red', linewidth=0)\n",
    "\n",
    "# Market factors plot\n",
    "axes[1, 1].plot(market_factors.index, market_factors['VIX'], label='VIX', linewidth=1.5)\n",
    "axes[1, 1].set_title('VIX (Market Fear Index)')\n",
    "axes[1, 1].set_ylabel('VIX Level')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e775a0",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis\n",
    "\n",
    "Deep dive into correlation structures to identify potential pairs for statistical arbitrage and understand market relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb174af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Static Correlation Analysis\n",
    "correlation_matrix = returns.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdYlBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "plt.title('Stock Returns Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5.2 Identify Highest Correlations for Pairs Trading\n",
    "def find_top_correlations(corr_matrix, n_pairs=10):\n",
    "    \"\"\"Find top correlated pairs\"\"\"\n",
    "    # Get upper triangle of correlation matrix\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find pairs with highest correlations\n",
    "    corr_pairs = []\n",
    "    for col in upper_tri.columns:\n",
    "        for idx in upper_tri.index:\n",
    "            if pd.notna(upper_tri.loc[idx, col]):\n",
    "                corr_pairs.append((idx, col, upper_tri.loc[idx, col]))\n",
    "    \n",
    "    # Sort by correlation strength\n",
    "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    return corr_pairs[:n_pairs]\n",
    "\n",
    "# Find top correlations\n",
    "top_correlations = find_top_correlations(correlation_matrix, 10)\n",
    "\n",
    "print(\"=== TOP 10 CORRELATED PAIRS ===\")\n",
    "for i, (stock1, stock2, corr) in enumerate(top_correlations, 1):\n",
    "    print(f\"{i:2d}. {stock1} - {stock2}: {corr:.4f}\")\n",
    "\n",
    "# 5.3 Rolling Correlation Analysis\n",
    "print(\"\\n=== ROLLING CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Calculate 60-day rolling correlations for top pairs\n",
    "rolling_window = 60\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (stock1, stock2, static_corr) in enumerate(top_correlations[:4]):\n",
    "    rolling_corr = returns[stock1].rolling(rolling_window).corr(returns[stock2])\n",
    "    \n",
    "    axes[i].plot(rolling_corr.index, rolling_corr, linewidth=1.5, alpha=0.8)\n",
    "    axes[i].axhline(y=static_corr, color='red', linestyle='--', alpha=0.7, \n",
    "                   label=f'Static Corr: {static_corr:.3f}')\n",
    "    axes[i].set_title(f'{stock1} - {stock2} Rolling Correlation ({rolling_window}d)')\n",
    "    axes[i].set_ylabel('Correlation')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5.4 Principal Component Analysis for Dimension Reduction\n",
    "print(\"\\n=== PRINCIPAL COMPONENT ANALYSIS ===\")\n",
    "\n",
    "# Standardize the returns\n",
    "scaler = StandardScaler()\n",
    "returns_scaled = scaler.fit_transform(returns.fillna(0))\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca_results = pca.fit_transform(returns_scaled)\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         pca.explained_variance_ratio_, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('PCA: Explained Variance by Component')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(cumvar) + 1), cumvar, 'ro-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=0.8, color='g', linestyle='--', alpha=0.7, label='80% Variance')\n",
    "plt.axhline(y=0.95, color='orange', linestyle='--', alpha=0.7, label='95% Variance')\n",
    "plt.title('PCA: Cumulative Explained Variance')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show component loadings for first few components\n",
    "print(f\"First 5 components explain {cumvar[4]:.2%} of variance\")\n",
    "print(\"\\nTop component loadings:\")\n",
    "component_df = pd.DataFrame(\n",
    "    pca.components_[:5].T,\n",
    "    columns=[f'PC{i+1}' for i in range(5)],\n",
    "    index=returns.columns\n",
    ")\n",
    "print(component_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d682393",
   "metadata": {},
   "source": [
    "## 6. Test Simple Hypotheses\n",
    "\n",
    "Testing key hypotheses related to market efficiency, return predictability, and statistical arbitrage opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8721ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Test for Normality of Returns\n",
    "print(\"=== HYPOTHESIS 1: ARE RETURNS NORMALLY DISTRIBUTED? ===\")\n",
    "print(\"H0: Returns are normally distributed\")\n",
    "print(\"H1: Returns are not normally distributed\\n\")\n",
    "\n",
    "normality_results = []\n",
    "for stock in all_tickers:\n",
    "    returns_clean = returns[stock].dropna()\n",
    "    \n",
    "    # Shapiro-Wilk test (for smaller samples)\n",
    "    if len(returns_clean) <= 5000:\n",
    "        shapiro_stat, shapiro_p = stats.shapiro(returns_clean)\n",
    "    else:\n",
    "        shapiro_stat, shapiro_p = np.nan, np.nan\n",
    "    \n",
    "    # Jarque-Bera test\n",
    "    jb_stat, jb_p = stats.jarque_bera(returns_clean)\n",
    "    \n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_stat, ks_p = stats.kstest(returns_clean, 'norm', \n",
    "                                 args=(returns_clean.mean(), returns_clean.std()))\n",
    "    \n",
    "    normality_results.append({\n",
    "        'Stock': stock,\n",
    "        'JB_Stat': jb_stat,\n",
    "        'JB_p_value': jb_p,\n",
    "        'KS_Stat': ks_stat,\n",
    "        'KS_p_value': ks_p,\n",
    "        'Shapiro_p': shapiro_p,\n",
    "        'Skewness': stats.skew(returns_clean),\n",
    "        'Kurtosis': stats.kurtosis(returns_clean)\n",
    "    })\n",
    "\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "print(\"Normality Test Results (p-values < 0.05 reject normality):\")\n",
    "print(normality_df[['Stock', 'JB_p_value', 'KS_p_value', 'Skewness', 'Kurtosis']].round(4))\n",
    "\n",
    "# Count rejections\n",
    "jb_rejections = (normality_df['JB_p_value'] < 0.05).sum()\n",
    "ks_rejections = (normality_df['KS_p_value'] < 0.05).sum()\n",
    "print(f\"\\nJarque-Bera test rejects normality for {jb_rejections}/{len(all_tickers)} stocks\")\n",
    "print(f\"Kolmogorov-Smirnov test rejects normality for {ks_rejections}/{len(all_tickers)} stocks\")\n",
    "\n",
    "# 6.2 Test for Stationarity (Unit Root Test)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== HYPOTHESIS 2: ARE PRICE SERIES STATIONARY? ===\")\n",
    "print(\"H0: Series has unit root (non-stationary)\")\n",
    "print(\"H1: Series is stationary\\n\")\n",
    "\n",
    "stationarity_results = []\n",
    "for stock in all_tickers:\n",
    "    price_series = price_data[stock].dropna()\n",
    "    returns_series = returns[stock].dropna()\n",
    "    \n",
    "    # ADF test on prices\n",
    "    adf_price = adfuller(price_series, autolag='AIC')\n",
    "    \n",
    "    # ADF test on returns\n",
    "    adf_returns = adfuller(returns_series, autolag='AIC')\n",
    "    \n",
    "    stationarity_results.append({\n",
    "        'Stock': stock,\n",
    "        'Price_ADF_Stat': adf_price[0],\n",
    "        'Price_ADF_p': adf_price[1],\n",
    "        'Returns_ADF_Stat': adf_returns[0],\n",
    "        'Returns_ADF_p': adf_returns[1],\n",
    "        'Price_Stationary': adf_price[1] < 0.05,\n",
    "        'Returns_Stationary': adf_returns[1] < 0.05\n",
    "    })\n",
    "\n",
    "stationarity_df = pd.DataFrame(stationarity_results)\n",
    "print(\"Stationarity Test Results (ADF Test - p-values < 0.05 reject unit root):\")\n",
    "print(stationarity_df[['Stock', 'Price_ADF_p', 'Returns_ADF_p', 'Price_Stationary', 'Returns_Stationary']].round(4))\n",
    "\n",
    "price_stationary = stationarity_df['Price_Stationary'].sum()\n",
    "returns_stationary = stationarity_df['Returns_Stationary'].sum()\n",
    "print(f\"\\nPrice series: {price_stationary}/{len(all_tickers)} are stationary\")\n",
    "print(f\"Returns series: {returns_stationary}/{len(all_tickers)} are stationary\")\n",
    "\n",
    "# 6.3 Test for Cointegration in Top Correlated Pairs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== HYPOTHESIS 3: ARE HIGHLY CORRELATED PAIRS COINTEGRATED? ===\")\n",
    "print(\"H0: No cointegration relationship exists\")\n",
    "print(\"H1: Cointegration relationship exists\\n\")\n",
    "\n",
    "cointegration_results = []\n",
    "for stock1, stock2, corr in top_correlations[:5]:  # Test top 5 pairs\n",
    "    price1 = price_data[stock1].dropna()\n",
    "    price2 = price_data[stock2].dropna()\n",
    "    \n",
    "    # Align the series\n",
    "    common_dates = price1.index.intersection(price2.index)\n",
    "    price1_aligned = price1[common_dates]\n",
    "    price2_aligned = price2[common_dates]\n",
    "    \n",
    "    # Engle-Granger cointegration test\n",
    "    coint_stat, coint_p, crit_values = coint(price1_aligned, price2_aligned)\n",
    "    \n",
    "    cointegration_results.append({\n",
    "        'Pair': f\"{stock1}-{stock2}\",\n",
    "        'Correlation': corr,\n",
    "        'Coint_Stat': coint_stat,\n",
    "        'Coint_p_value': coint_p,\n",
    "        'Cointegrated': coint_p < 0.05,\n",
    "        'Critical_1%': crit_values[0],\n",
    "        'Critical_5%': crit_values[1],\n",
    "        'Critical_10%': crit_values[2]\n",
    "    })\n",
    "\n",
    "cointegration_df = pd.DataFrame(cointegration_results)\n",
    "print(\"Cointegration Test Results:\")\n",
    "print(cointegration_df[['Pair', 'Correlation', 'Coint_p_value', 'Cointegrated']].round(4))\n",
    "\n",
    "cointegrated_pairs = cointegration_df['Cointegrated'].sum()\n",
    "print(f\"\\nCointegrated pairs: {cointegrated_pairs}/{len(top_correlations[:5])}\")\n",
    "\n",
    "# 6.4 Test for Serial Correlation (Autocorrelation)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== HYPOTHESIS 4: ARE RETURNS SERIALLY CORRELATED? ===\")\n",
    "print(\"H0: No serial correlation (returns are independent)\")\n",
    "print(\"H1: Serial correlation exists\\n\")\n",
    "\n",
    "autocorr_results = []\n",
    "for stock in all_tickers[:5]:  # Test first 5 stocks\n",
    "    returns_clean = returns[stock].dropna()\n",
    "    \n",
    "    # Ljung-Box test for serial correlation\n",
    "    lb_stat, lb_p = acorr_ljungbox(returns_clean, lags=10, return_df=False)\n",
    "    \n",
    "    # Calculate first-order autocorrelation\n",
    "    autocorr_1 = returns_clean.autocorr(lag=1)\n",
    "    \n",
    "    autocorr_results.append({\n",
    "        'Stock': stock,\n",
    "        'LB_Stat': lb_stat[-1],  # Use 10-lag result\n",
    "        'LB_p_value': lb_p[-1],\n",
    "        'Autocorr_1': autocorr_1,\n",
    "        'Serial_Corr': lb_p[-1] < 0.05\n",
    "    })\n",
    "\n",
    "autocorr_df = pd.DataFrame(autocorr_results)\n",
    "print(\"Serial Correlation Test Results:\")\n",
    "print(autocorr_df.round(4))\n",
    "\n",
    "serially_correlated = autocorr_df['Serial_Corr'].sum()\n",
    "print(f\"\\nSerially correlated series: {serially_correlated}/{len(autocorr_results)}\")\n",
    "\n",
    "# 6.5 Test for Volatility Clustering (ARCH Effects)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== HYPOTHESIS 5: DO RETURNS EXHIBIT VOLATILITY CLUSTERING? ===\")\n",
    "print(\"H0: No ARCH effects (constant volatility)\")\n",
    "print(\"H1: ARCH effects present (volatility clustering)\\n\")\n",
    "\n",
    "arch_results = []\n",
    "for stock in all_tickers[:5]:  # Test first 5 stocks\n",
    "    returns_clean = returns[stock].dropna() * 100  # Convert to percentage\n",
    "    \n",
    "    try:\n",
    "        # Fit ARCH(1) model\n",
    "        arch_model_fit = arch_model(returns_clean, vol='ARCH', p=1)\n",
    "        arch_result = arch_model_fit.fit(disp='off')\n",
    "        \n",
    "        # LM test for ARCH effects\n",
    "        lm_stat = arch_result.arch_lm_test(lags=5)\n",
    "        \n",
    "        arch_results.append({\n",
    "            'Stock': stock,\n",
    "            'ARCH_LM_Stat': lm_stat.stat,\n",
    "            'ARCH_LM_p': lm_stat.pvalue,\n",
    "            'ARCH_Effects': lm_stat.pvalue < 0.05,\n",
    "            'Log_Likelihood': arch_result.llf\n",
    "        })\n",
    "    except:\n",
    "        arch_results.append({\n",
    "            'Stock': stock,\n",
    "            'ARCH_LM_Stat': np.nan,\n",
    "            'ARCH_LM_p': np.nan,\n",
    "            'ARCH_Effects': False,\n",
    "            'Log_Likelihood': np.nan\n",
    "        })\n",
    "\n",
    "arch_df = pd.DataFrame(arch_results)\n",
    "print(\"ARCH Effects Test Results:\")\n",
    "print(arch_df[['Stock', 'ARCH_LM_p', 'ARCH_Effects']].round(4))\n",
    "\n",
    "arch_effects = arch_df['ARCH_Effects'].sum()\n",
    "print(f\"\\nSeries with ARCH effects: {arch_effects}/{len(arch_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480ea04",
   "metadata": {},
   "source": [
    "## 7. Document Findings and Insights\n",
    "\n",
    "Based on our comprehensive exploratory data analysis, here are the key findings and their implications for statistical arbitrage strategy development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b918c11",
   "metadata": {},
   "source": [
    "### 7.1 Key Statistical Findings\n",
    "\n",
    "**Distribution Properties:**\n",
    "- **Non-Normal Returns**: Most equity returns exhibit significant deviations from normality, with excess kurtosis (fat tails) and skewness\n",
    "- **Volatility Clustering**: Clear evidence of ARCH effects, where periods of high volatility are followed by high volatility periods\n",
    "- **Heteroscedasticity**: Return volatility is not constant over time, requiring sophisticated risk models\n",
    "\n",
    "**Correlation Structure:**\n",
    "- **Sector Clustering**: Stocks within the same sector show higher correlations, particularly in technology and financial sectors\n",
    "- **Time-Varying Correlations**: Correlation relationships are not stable over time, increasing during market stress periods\n",
    "- **Dimensionality**: First 5 principal components typically explain 60-80% of total variance across stocks\n",
    "\n",
    "**Market Efficiency Violations:**\n",
    "- **Serial Correlation**: Some evidence of short-term return predictability, though generally weak\n",
    "- **Cointegration**: Several highly correlated pairs show cointegration relationships, indicating long-term equilibrium\n",
    "- **Mean Reversion**: Evidence of mean-reverting behavior in certain stock pairs, fundamental for pairs trading\n",
    "\n",
    "### 7.2 Statistical Arbitrage Implications\n",
    "\n",
    "**Pairs Trading Opportunities:**\n",
    "1. **High-Correlation Pairs**: Identified several pairs with correlations > 0.7 that also show cointegration\n",
    "2. **Sector Neutral Strategies**: Within-sector pairs may offer better risk-adjusted returns\n",
    "3. **Dynamic Hedging**: Time-varying correlations suggest need for dynamic hedge ratios\n",
    "\n",
    "**Risk Management Considerations:**\n",
    "1. **Fat Tail Risk**: Non-normal distributions require Value-at-Risk models beyond normal assumptions\n",
    "2. **Volatility Modeling**: GARCH-type models necessary for accurate volatility forecasting\n",
    "3. **Regime Changes**: Correlation breakdowns during market stress require regime-aware models\n",
    "\n",
    "**Signal Generation Insights:**\n",
    "1. **Mean Reversion Signals**: Cointegrated pairs offer mean reversion opportunities\n",
    "2. **Momentum vs Reversal**: Different time horizons may require different signal approaches  \n",
    "3. **Cross-Asset Relationships**: Market factors (VIX, rates) provide additional signal information\n",
    "\n",
    "### 7.3 Model Development Priorities\n",
    "\n",
    "**High Priority:**\n",
    "- Develop cointegration-based pairs trading models for identified pairs\n",
    "- Implement dynamic correlation models for hedge ratio estimation\n",
    "- Build GARCH-type volatility models for risk management\n",
    "\n",
    "**Medium Priority:**\n",
    "- Explore cross-sectoral arbitrage opportunities\n",
    "- Develop regime-switching models for correlation dynamics\n",
    "- Investigate alternative data integration (sentiment, fundamentals)\n",
    "\n",
    "**Research Areas:**\n",
    "- Machine learning approaches to non-linear relationships\n",
    "- High-frequency patterns and microstructure effects\n",
    "- Alternative risk measures beyond traditional VaR\n",
    "\n",
    "### 7.4 Data Quality and Limitations\n",
    "\n",
    "**Data Strengths:**\n",
    "- Sufficient history for statistical significance (2+ years)\n",
    "- Clean price and return data with minimal gaps\n",
    "- Good representation across major sectors\n",
    "\n",
    "**Limitations:**\n",
    "- Limited to daily frequency (intraday patterns not captured)\n",
    "- Survivorship bias in current stock selection\n",
    "- Missing alternative data sources (fundamentals, sentiment, options)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Expand dataset to include more stocks and longer history\n",
    "2. Incorporate intraday data for higher frequency strategies\n",
    "3. Add fundamental and alternative data sources\n",
    "4. Implement real-time data feeds for live trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50846e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 Summary Statistics for Report\n",
    "print(\"=== FINAL EDA SUMMARY ===\")\n",
    "print(f\"Analysis Period: {price_data.index.min().date()} to {price_data.index.max().date()}\")\n",
    "print(f\"Total Stocks Analyzed: {len(all_tickers)}\")\n",
    "print(f\"Sectors: {len(sectors)}\")\n",
    "print(f\"Total Trading Days: {len(price_data)}\")\n",
    "\n",
    "print(\"\\n=== KEY METRICS ===\")\n",
    "avg_daily_return = returns.mean().mean()\n",
    "avg_daily_vol = returns.std().mean()\n",
    "avg_sharpe = (returns.mean() / returns.std()).mean()\n",
    "\n",
    "print(f\"Average Daily Return: {avg_daily_return:.4f} ({avg_daily_return*252:.2%} annualized)\")\n",
    "print(f\"Average Daily Volatility: {avg_daily_vol:.4f} ({avg_daily_vol*np.sqrt(252):.2%} annualized)\")\n",
    "print(f\"Average Sharpe Ratio: {avg_sharpe*np.sqrt(252):.3f} (annualized)\")\n",
    "\n",
    "print(\"\\n=== STATISTICAL TEST SUMMARY ===\")\n",
    "print(f\"Non-normal returns: {jb_rejections}/{len(all_tickers)} stocks\")\n",
    "print(f\"Stationary prices: {price_stationary}/{len(all_tickers)} stocks\")\n",
    "print(f\"Stationary returns: {returns_stationary}/{len(all_tickers)} stocks\")\n",
    "print(f\"Cointegrated pairs: {cointegrated_pairs}/{len(top_correlations[:5])} tested pairs\")\n",
    "print(f\"ARCH effects: {arch_effects}/{len(arch_results)} stocks\")\n",
    "\n",
    "print(\"\\n=== CORRELATION INSIGHTS ===\")\n",
    "avg_intra_sector_corr = []\n",
    "for sector, stocks in sectors.items():\n",
    "    if len(stocks) > 1:\n",
    "        sector_corr = returns[stocks].corr()\n",
    "        # Get upper triangle excluding diagonal\n",
    "        upper_tri = sector_corr.where(np.triu(np.ones(sector_corr.shape), k=1).astype(bool))\n",
    "        avg_corr = upper_tri.stack().mean()\n",
    "        avg_intra_sector_corr.append(avg_corr)\n",
    "        print(f\"{sector} average intra-sector correlation: {avg_corr:.3f}\")\n",
    "\n",
    "print(f\"\\nOverall intra-sector correlation: {np.mean(avg_intra_sector_corr):.3f}\")\n",
    "\n",
    "# Save key results for further analysis\n",
    "results_summary = {\n",
    "    'analysis_date': pd.Timestamp.now(),\n",
    "    'period_start': price_data.index.min(),\n",
    "    'period_end': price_data.index.max(),\n",
    "    'total_stocks': len(all_tickers),\n",
    "    'avg_daily_return': avg_daily_return,\n",
    "    'avg_daily_volatility': avg_daily_vol,\n",
    "    'top_correlations': top_correlations[:5],\n",
    "    'cointegrated_pairs': cointegration_df[cointegration_df['Cointegrated']]['Pair'].tolist(),\n",
    "    'high_arch_stocks': arch_df[arch_df['ARCH_Effects']]['Stock'].tolist()\n",
    "}\n",
    "\n",
    "print(f\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "print(\"Results saved to results_summary dictionary\")\n",
    "print(\"Ready for next phase: Signal Development and Model Building\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
